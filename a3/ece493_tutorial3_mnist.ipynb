{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First import pytorch requirements\n",
    "\n",
    "Usually you important everything at the top of your python file or jupyter notebook but I have explanations as we go through this and only import the minimum requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pytorch library\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load MNIST and verify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:04, 2053126.81it/s]                             \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-images-idx3-ubyte.gz to data/MNIST/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 53075.95it/s]            \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/train-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:01, 1226034.07it/s]                             \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-images-idx3-ubyte.gz to data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 31556.31it/s]            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/MNIST/raw/t10k-labels-idx1-ubyte.gz to data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "Number of training examples: 60000\n",
      "Each with one image and one ground truth (gt) label: 2\n",
      "One image and one label (number)\n",
      "<PIL.Image.Image image mode=L size=28x28 at 0x7FEE6C685F10>\n",
      "5\n",
      "Lets take a look at the image size\n",
      "(28, 28)\n",
      "Lastly let's verify that the image and gt label match\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fee69e0b9d0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torchvision is a computer vision library that pytorch created for common datasets\n",
    "# https://pytorch.org/docs/stable/torchvision\n",
    "import torchvision\n",
    "# import MNIST loader\n",
    "from torchvision.datasets import MNIST\n",
    "\n",
    "# https://pytorch.org/docs/stable/torchvision/datasets.html#mnist\n",
    "# torchvision.datasets.MNIST(root, train=True, transform=None, target_transform=None, download=False)\n",
    "mnist_train_data = MNIST('data', train=True, download=True)\n",
    "\n",
    "print(\"Number of training examples:\", len(mnist_train_data))\n",
    "print(\"Each with one image and one ground truth (gt) label:\", len(mnist_train_data[0]))\n",
    "\n",
    "print(\"One image and one label (number)\")\n",
    "print(mnist_train_data[0][0])\n",
    "print(mnist_train_data[0][1])\n",
    "\n",
    "print(\"Lets take a look at the image size\")\n",
    "print(mnist_train_data[0][0].size)\n",
    "\n",
    "print(\"Lastly let's verify that the image and gt label match\")\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "img, label = mnist_train_data[0]\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(label)\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create training transform and data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import data transforms\n",
    "# https://pytorch.org/docs/stable/torchvision/transforms.html\n",
    "# Take note of transforms.ToPILImage and transforms.ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# We want to give our neural network more exmaples. What transforms make sense for this data?\n",
    "# ColorJitter? Nope, the number is black and white\n",
    "# Crop or Flip? Nope, that could remove parts of the number or make it unreadable\n",
    "# RandomRotation? Yes, numbers can be slightly slanted\n",
    "# Normalize? Yes, you should always normalize\n",
    "\n",
    "mnist_train_tf = transforms.Compose([\n",
    "    transforms.RandomRotation([-10, 10]),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Supplied with dataset (mean, std)\n",
    "])\n",
    "\n",
    "# Let's test out the transform (without toTensor)\n",
    "img_tfed = transforms.RandomRotation([90, 90])(img)\n",
    "# Display the image\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(label)\n",
    "ax1.imshow(img_tfed, cmap=\"gray\")\n",
    "\n",
    "# https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# reimport MNIST with transform\n",
    "mnist_train_data_transformed = MNIST('data', train=True, download=True, transform=mnist_train_tf)\n",
    "\n",
    "# epoch = one iteration over the entire dataset\n",
    "# batch size = samples per batch within an epoch\n",
    "# shuffle = Set to True if your data is ordered\n",
    "# num_workers = number of subprocess to use when dataloading, set higher if you have many cpu cores\n",
    "train_loader = DataLoader(mnist_train_data_transformed, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Important concepts to remember before creating the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kernel - Convolution: https://en.wikipedia.org/wiki/Kernel_(image_processing)#Convolution\n",
    "- Weighted sum around a pixel\n",
    "- nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "- nn.Conv2d(1, 32, 3, 1)\n",
    "\n",
    "Activation functions:https://en.wikipedia.org/wiki/Activation_function\n",
    "- Logistic - https://en.wikipedia.org/wiki/Logistic_function\n",
    "- ReLU - https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "Pooling: https://pytorch.org/docs/stable/nn.functional.html#pooling-functions\n",
    "- Downsample the feature map\n",
    "- Notable pooling functions: average and max\n",
    "\n",
    "Dropout: https://pytorch.org/docs/stable/nn.html#dropout-layers\n",
    "- Promote independence between feature maps\n",
    "- The dropout paper https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf\n",
    "\n",
    "Fully connected layers\n",
    "- First flatten the tensor\n",
    "- Goal is to classify the image based on given feature maps\n",
    "- Eventual output is the number of classes\n",
    "\n",
    "Final output layer\n",
    "- We want each class index to have probability\n",
    "- Can we use softmax? Takes vector and makes sum 1. https://pytorch.org/docs/stable/nn.html#torch.nn.Softmax no it has some issues with NLL loss https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/\n",
    "- log softmax https://pytorch.org/docs/stable/nn.html#torch.nn.LogSoftmax\n",
    "\n",
    "\n",
    "Visualizing and Understanding Convolutional Networks\n",
    "https://arxiv.org/abs/1311.2901\n",
    "- Lines -> Lines/Curves/Corners -> Patterns -> Objects\n",
    "\n",
    "Even more detail:\n",
    "\n",
    "http://cs231n.github.io/convolutional-networks/\n",
    "\n",
    "https://en.wikipedia.org/wiki/Convolutional_neural_network#Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural network layers (Conv2d, Linear, etc.) that will be trained\n",
    "# https://pytorch.org/docs/stable/nn.html\n",
    "import torch.nn as nn\n",
    "# Many functions (convolution, pooling, activation, etc.)\n",
    "# https://pytorch.org/docs/stable/nn.functional.html\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# This NN implementation is from the pytorch github\n",
    "# https://github.com/pytorch/examples/blob/master/mnist/main.py\n",
    "\n",
    "# all nn layers defined and stored in vars within __init__\n",
    "# https://pytorch.org/docs/stable/nn.html#torch.nn.Conv2d\n",
    "# torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1)\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout2d(0.25)\n",
    "        self.dropout2 = nn.Dropout2d(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # comments were created by: print(x.shape)\n",
    "        # initial x shape: torch.Size([4, 1, 28, 28])\n",
    "        # 4 = batch_size\n",
    "        # 1 = number of channels (Greyscale will have 1, RGB with have 3)\n",
    "        # 28 = height\n",
    "        # 28 = width\n",
    "        x = self.conv1(x)       # torch.Size([4, 32, 26, 26])\n",
    "        x = F.relu(x)           # torch.Size([4, 32, 26, 26])\n",
    "        x = self.conv2(x)       # torch.Size([4, 64, 24, 24])\n",
    "        x = F.max_pool2d(x, 2)  # torch.Size([4, 64, 12, 12])\n",
    "        x = self.dropout1(x)    # torch.Size([4, 64, 12, 12])\n",
    "        x = torch.flatten(x, 1) # torch.Size([4, 9216])\n",
    "        x = self.fc1(x)         # torch.Size([4, 128])\n",
    "        x = F.relu(x)           # torch.Size([4, 128])\n",
    "        x = self.dropout2(x)    # torch.Size([4, 128])\n",
    "        x = self.fc2(x)         # torch.Size([4, 10])\n",
    "        output = F.log_softmax(x, dim=1) # torch.Size([4, 10])\n",
    "        return output\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "# Overide device when testing on CPU\n",
    "# device = 'cpu'\n",
    "\n",
    "net = Net().to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Various optimization algorithms are in:\n",
    "# https://pytorch.org/docs/stable/optim.html\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# NLLLoss is used when you expect 1 of N classes\n",
    "# https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html\n",
    "criterion = nn.NLLLoss()\n",
    "\n",
    "# Stochastic Gradient Descent\n",
    "# https://pytorch.org/docs/stable/optim.html#torch.optim.SGD\n",
    "# torch.optim.SGD(params, lr=<required parameter>, momentum=0, dampening=0, weight_decay=0, nesterov=False)\n",
    "# lr is the learning rate, how much to change weights based on current values\n",
    "# momentum will change how much to change weights based on previous changes\n",
    "# https://www.quora.com/What-is-the-difference-between-momentum-and-learning-rate\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,  2000] loss: 0.658\n",
      "[1,  4000] loss: 0.257\n",
      "[1,  6000] loss: 0.205\n",
      "[1,  8000] loss: 0.167\n",
      "[1, 10000] loss: 0.160\n",
      "[1, 12000] loss: 0.134\n",
      "[1, 14000] loss: 0.146\n",
      "[2,  2000] loss: 0.131\n",
      "[2,  4000] loss: 0.117\n",
      "[2,  6000] loss: 0.107\n",
      "[2,  8000] loss: 0.109\n",
      "[2, 10000] loss: 0.104\n",
      "[2, 12000] loss: 0.120\n",
      "[2, 14000] loss: 0.106\n",
      "[3,  2000] loss: 0.100\n",
      "[3,  4000] loss: 0.097\n",
      "[3,  6000] loss: 0.093\n",
      "[3,  8000] loss: 0.090\n",
      "[3, 10000] loss: 0.102\n",
      "[3, 12000] loss: 0.104\n",
      "[3, 14000] loss: 0.081\n",
      "[4,  2000] loss: 0.084\n",
      "[4,  4000] loss: 0.076\n",
      "[4,  6000] loss: 0.078\n",
      "[4,  8000] loss: 0.081\n",
      "[4, 10000] loss: 0.078\n",
      "[4, 12000] loss: 0.091\n",
      "[4, 14000] loss: 0.081\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(4):  # loop over the dataset twice\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(data)\n",
    "        loss = criterion(outputs, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if batch_idx % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, batch_idx + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Test loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data loading should not modify the image \n",
    "mnist_test_tf = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)) # Supplied with dataset (mean, std)\n",
    "])\n",
    "mnist_test_data = MNIST('data', train=False, download=True, transform=mnist_test_tf)\n",
    "\n",
    "test_loader = DataLoader(mnist_test_data, batch_size=4, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Evaluation Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0351, Accuracy: 9885/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# View some results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find one correctly classified and one misclassified\n",
    "\n",
    "found_bad = False\n",
    "found_good = False\n",
    "bad_example = None\n",
    "good_example = None\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, target in test_loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        output = net(data)\n",
    "        pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "        \n",
    "        for i in range (0,len(target)):\n",
    "            if not found_good and target[i] == pred[i]:\n",
    "                good_example = transforms.ToPILImage()(data[i].cpu()), target[i].cpu().numpy(), pred[i][0].cpu().numpy()\n",
    "                found_good = True\n",
    "            if not found_bad and target[i] != pred[i]:\n",
    "                bad_example = transforms.ToPILImage()(data[i].cpu()), target[i].cpu().numpy(), pred[i][0].cpu().numpy()\n",
    "                found_bad = True\n",
    "        \n",
    "        if found_bad and found_good:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1fbc6908d0>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, target_label, pred_label = good_example\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(\"Predicted label: \" + str(pred_label) + \" Target label: \" + str(target_label))\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1fcdfcf690>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 72x72 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img, target_label, pred_label = bad_example\n",
    "\n",
    "fig = plt.figure(figsize=(1,1))\n",
    "ax1 = fig.add_subplot(1,1,1)\n",
    "plt.title(\"Predicted label: \" + str(pred_label) + \" Target label: \" + str(target_label))\n",
    "ax1.imshow(img, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
